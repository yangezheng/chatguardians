{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "SexComment = load_dataset(\"csv\", data_files=\"SexComment.csv\")['train']\n",
    "SexComment_dataset = SexComment.remove_columns(['index','weibo_id','gender','location','like','date'])\n",
    "SexComment_dataset = SexComment_dataset.rename_column('comment_text','text')\n",
    "SexComment_dataset = SexComment_dataset.rename_column('label','label_sexist')\n",
    "SexComment_dataset = SexComment_dataset.rename_column('text','text_cn')\n",
    "\n",
    " \n",
    "dataset = SexComment_dataset.class_encode_column(\"label_sexist\")\n",
    "\n",
    "## remove long rows with long text \n",
    "dataset = dataset.filter(lambda x: len(x['text_cn'])<200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(model=\"Helsinki-NLP/opus-mt-zh-en\", device=device)\n",
    "# pipe = pipeline(model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "\n",
    "\n",
    "def trans_CN_to_EN(cn: str) -> str:\n",
    "    return pipe(cn)[0]['translation_text']\n",
    "\n",
    "def add_text(row):\n",
    "    row['text'] = trans_CN_to_EN(row['text_cn'])\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## process token\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "\n",
    "# def tokenization(example):\n",
    "#     return tokenizer(example[\"text_cn\"])\n",
    "\n",
    "# dataset = dataset.map(tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1e9d73948c444ca29b71ae6ca9dcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8571 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzheng/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a90b87e2ba4d37afe9dac51debb33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/8571 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text_cn', 'label_sexist', 'category', 'target', 'text'],\n",
      "        num_rows: 6941\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text_cn', 'label_sexist', 'category', 'target', 'text'],\n",
      "        num_rows: 772\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text_cn', 'label_sexist', 'category', 'target', 'text'],\n",
      "        num_rows: 858\n",
      "    })\n",
      "})\n",
      "Token read successfully\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/yzheng/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea6e0722614411b8a9fe9569327e5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35c34a856dc46d4b48a13415d1c04f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91a01d1c6f54f8cb3801092ab3e3d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d880a9d4c354d8fb4e3124b6c3e88d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ed519df3d44b17bc1d547011a071e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad6c97c58da42b89008f57b28034729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Translate the dataset\n",
    "dataset = dataset.map(add_text)\n",
    "\n",
    "\n",
    "## Split the dataset\n",
    "\n",
    "from datasets import ClassLabel, Value\n",
    "\n",
    "new_features = dataset.features.copy()\n",
    "new_features['label_sexist'] = ClassLabel(names=[\"not sexist\", \"sexist\"])\n",
    "new_dataset = dataset.cast(new_features)\n",
    "\n",
    "split_dataset = new_dataset.train_test_split(0.1,stratify_by_column='label_sexist')\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "split_again = train_dataset.train_test_split(0.1,stratify_by_column='label_sexist')\n",
    "train_dataset = split_again['train']\n",
    "validation_dataset = split_again['test']\n",
    "\n",
    "## combine the dataset\n",
    "\n",
    "from datasets import DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset,\n",
    "})\n",
    "print(dataset_dict)\n",
    "\n",
    "\n",
    "## Upload the dataset\n",
    "\n",
    "import json\n",
    "\n",
    "file_path = \"credentials.json\"\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    token_data = json.load(json_file)\n",
    "\n",
    "token = token_data.get(\"huggingface_token\")\n",
    "if token:\n",
    "    print(f\"Token read successfully\")\n",
    "else:\n",
    "    print(\"Error: Token not found in the JSON file.\")\n",
    "    \n",
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login(token)\n",
    "\n",
    "dataset_dict.push_to_hub(\"yangezheng/SWSR-SexComment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"credentials.json\"\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    token_data = json.load(json_file)\n",
    "\n",
    "token = token_data.get(\"huggingface_token\")\n",
    "if token:\n",
    "    print(f\"Token read successfully\")\n",
    "else:\n",
    "    print(\"Error: Token not found in the JSON file.\")\n",
    "    \n",
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login(token)\n",
    "\n",
    "dataset_dict.push_to_hub(\"yangezheng/CMSB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
