{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import ClassLabel\n",
    "\n",
    "SexWeibo = load_dataset(\"csv\", data_files=\"SexWeibo.csv\")['train']\n",
    "SexWeibo_dataset = SexWeibo.remove_columns(['weibo_id','user_gender','user_location','user_following','user_follower','weibo_like','weibo_repost', 'weibo_comment', 'weibo_date'])\n",
    "SexWeibo_dataset = SexWeibo_dataset.rename_column('weibo_text','text_cn')\n",
    "# SexWeibo_dataset = SexWeibo_dataset.filter(lambda x: x['keyword'] is not None and x['keyword'].find('厌女') != -1)\n",
    "\n",
    "sexism_column = ['not sexist'] * len(SexWeibo_dataset)\n",
    "SexWeibo_dataset = SexWeibo_dataset.add_column('label_sexist', sexism_column)\n",
    "\n",
    "def identify_sexist(row):\n",
    "    if row['keyword'] is not None and row['keyword'].find('厌女') != -1:\n",
    "        row['label_sexist'] = 'sexist'\n",
    "    return row\n",
    "\n",
    "SexWeibo_dataset = SexWeibo_dataset.map(identify_sexist, batched=False)\n",
    "\n",
    "dataset = SexWeibo_dataset.class_encode_column(\"label_sexist\")\n",
    "\n",
    "# ## remove long rows with long text \n",
    "dataset = dataset.filter(lambda x: len(x['text_cn'])<200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text_cn', 'keyword', 'label_sexist'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(model=\"Helsinki-NLP/opus-mt-zh-en\", device=device)\n",
    "# pipe = pipeline(model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "\n",
    "\n",
    "def trans_CN_to_EN(cn: str) -> str:\n",
    "    return pipe(cn)[0]['translation_text']\n",
    "\n",
    "def add_text(row):\n",
    "    row['text'] = trans_CN_to_EN(row['text_cn'])\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e8417b385446d8a427dfe34020d3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text_cn', 'keyword', 'label_sexist', 'text'],\n",
      "        num_rows: 583\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text_cn', 'keyword', 'label_sexist', 'text'],\n",
      "        num_rows: 65\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text_cn', 'keyword', 'label_sexist', 'text'],\n",
      "        num_rows: 72\n",
      "    })\n",
      "})\n",
      "Token read successfully\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/yzheng/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9278eb427bde4884aa2b6c2713c07754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44cc65802394857843147bcefcea67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680709afd93b420da35adaa762a6487b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf6cd56294a469087502110590efb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b71e14779c494b9b13fc45e07029a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d458a259058a4517a459b2d355cf0023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Translate the dataset\n",
    "new_dataset = dataset.map(add_text)\n",
    "\n",
    "\n",
    "## Split the dataset\n",
    "\n",
    "# new_features = dataset.features.copy()\n",
    "# new_features['label_sexist'] = ClassLabel(names=[\"not sexist\", \"sexist\"])\n",
    "# new_dataset = dataset.cast(new_features)\n",
    "\n",
    "split_dataset = new_dataset.train_test_split(0.1,stratify_by_column='label_sexist')\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "split_again = train_dataset.train_test_split(0.1,stratify_by_column='label_sexist')\n",
    "train_dataset = split_again['train']\n",
    "validation_dataset = split_again['test']\n",
    "\n",
    "## combine the dataset\n",
    "\n",
    "from datasets import DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset,\n",
    "})\n",
    "print(dataset_dict)\n",
    "\n",
    "\n",
    "## Upload the dataset\n",
    "\n",
    "import json\n",
    "\n",
    "file_path = \"credentials.json\"\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    token_data = json.load(json_file)\n",
    "\n",
    "token = token_data.get(\"huggingface_token\")\n",
    "if token:\n",
    "    print(f\"Token read successfully\")\n",
    "else:\n",
    "    print(\"Error: Token not found in the JSON file.\")\n",
    "    \n",
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login(token)\n",
    "\n",
    "dataset_dict.push_to_hub(\"yangezheng/SWSR-SexWeibo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"credentials.json\"\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    token_data = json.load(json_file)\n",
    "\n",
    "token = token_data.get(\"huggingface_token\")\n",
    "if token:\n",
    "    print(f\"Token read successfully\")\n",
    "else:\n",
    "    print(\"Error: Token not found in the JSON file.\")\n",
    "    \n",
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login(token)\n",
    "\n",
    "dataset_dict.push_to_hub(\"yangezheng/CMSB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
