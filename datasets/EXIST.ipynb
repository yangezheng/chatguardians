{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "EXIST_train = load_dataset(\"csv\", data_files=\"EXIST2021_training.tsv\", sep=\"\\t\")['train']\n",
    "EXIST_test = load_dataset(\"csv\", data_files=\"EXIST2021_test_labeled.tsv\", sep=\"\\t\")['train']\n",
    "\n",
    "EXIST_train = EXIST_train.remove_columns(['test_case', 'id', 'source', 'language'])\n",
    "EXIST_test = EXIST_test.remove_columns(['test_case', 'id', 'source', 'language'])\n",
    "\n",
    "EXIST_train = EXIST_train.rename_column('task1','label_sexist')\n",
    "EXIST_test = EXIST_test.rename_column('task1','label_sexist')\n",
    "\n",
    "EXIST_train = EXIST_train.rename_column('text','text_original')\n",
    "EXIST_test = EXIST_test.rename_column('text','text_original')\n",
    "\n",
    "EXIST_train = EXIST_train.class_encode_column(\"label_sexist\")\n",
    "EXIST_test = EXIST_test.class_encode_column(\"label_sexist\")\n",
    "\n",
    "\n",
    "EXIST_train = EXIST_train.filter(lambda x: len(x['text_original'])<450)\n",
    "EXIST_test = EXIST_test.filter(lambda x: len(x['text_original'])<450)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_original': 'Now, back to these women, the brave and the beautiful, @Clare_Crawley and @tayshia. These bad ass babes, are deserve so much credit for how this season has gone. As a woman, Iâ€™ve learned so much from them and feel more empowered to expect more in future relationships.',\n",
       " 'label_sexist': 0,\n",
       " 'task2': 'non-sexist'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXIST_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_original': '@iilovegrapes He sounds like as ass, and very condescending.',\n",
       " 'label_sexist': 0,\n",
       " 'task2': 'non-sexist'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXIST_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(model=\"Helsinki-NLP/opus-mt-es-en\", device=device)\n",
    "# pipe = pipeline(model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "\n",
    "\n",
    "def trans_ES_to_EN(cn: str) -> str:\n",
    "    return pipe(cn,batch_size=8, truncation=\"only_first\")[0]['translation_text']\n",
    "\n",
    "def add_text(row):\n",
    "    row['text'] = trans_ES_to_EN(row['text_original'])\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341fceca0b5f4e90bdc7a6102e4cffb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzheng/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text_original', 'label_sexist', 'task2', 'text'],\n",
      "        num_rows: 5564\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text_original', 'label_sexist', 'task2', 'text'],\n",
      "        num_rows: 1391\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text_original', 'label_sexist', 'task2', 'text'],\n",
      "        num_rows: 4311\n",
      "    })\n",
      "})\n",
      "Token read successfully\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/yzheng/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cf7864c7224decaab5bcfa41e5068d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92d1faf1ac34e61a38a2036ae9d938f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a45a3287f945dab04309cdb7b07653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf0085bbfe54a6eae7683b785ed1962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2e4a4453b84aff82b12def06c9c720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9981f7991b14700934181faf174df30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "new_features = EXIST_train.features.copy()\n",
    "\n",
    "new_features['label_sexist'] = ClassLabel(names=[\"not sexist\", \"sexist\"])\n",
    "EXIST_train = EXIST_train.cast(new_features)\n",
    "EXIST_test = EXIST_test.cast(new_features)\n",
    "\n",
    "\n",
    "## Translate the dataset\n",
    "EXIST_train = EXIST_train.map(add_text)\n",
    "EXIST_test = EXIST_test.map(add_text)\n",
    "\n",
    "\n",
    "\n",
    "## Split the dataset\n",
    "\n",
    "split_dataset = EXIST_train.train_test_split(0.2,stratify_by_column='label_sexist')\n",
    "train_dataset = split_dataset['train']\n",
    "validation_dataset = split_dataset['test']\n",
    "test_dataset = EXIST_test\n",
    "\n",
    "## combine the dataset\n",
    "\n",
    "from datasets import DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset,\n",
    "})\n",
    "print(dataset_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Upload the dataset\n",
    "\n",
    "import json\n",
    "\n",
    "file_path = \"credentials.json\"\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    token_data = json.load(json_file)\n",
    "\n",
    "token = token_data.get(\"huggingface_token\")\n",
    "if token:\n",
    "    print(f\"Token read successfully\")\n",
    "else:\n",
    "    print(\"Error: Token not found in the JSON file.\")\n",
    "    \n",
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login(token)\n",
    "\n",
    "dataset_dict.push_to_hub(\"yangezheng/EXIST2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token read successfully\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/yzheng/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a900889cfbbc4ab9b0fdd0dc36af8821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954a4b92bdcd45748284d084fba53a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeffba0ebb954a91934cf9c97ddec83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa37c7d057649d2920044af2a82c833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2047cfd934c548babe185a0f1797fc50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68781554a4d547df9a38561c8cc7d231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "## Upload the dataset\n",
    "\n",
    "import json\n",
    "\n",
    "file_path = \"credentials.json\"\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    token_data = json.load(json_file)\n",
    "\n",
    "token = token_data.get(\"huggingface_token\")\n",
    "if token:\n",
    "    print(f\"Token read successfully\")\n",
    "else:\n",
    "    print(\"Error: Token not found in the JSON file.\")\n",
    "    \n",
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login(token)\n",
    "\n",
    "dataset_dict.push_to_hub(\"yangezheng/EXIST2021\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
