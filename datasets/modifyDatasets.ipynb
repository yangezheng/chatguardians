{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUM-NLP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "raw_datasets = load_dataset(\"tum-nlp/sexism-socialmedia-balanced\")['train']\n",
    "raw_datasets = raw_datasets.class_encode_column(\"label_sexist\")\n",
    "split_dataset = raw_datasets.train_test_split(0.1,stratify_by_column='label_sexist')\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "split_again = train_dataset.train_test_split(0.1,stratify_by_column='label_sexist')\n",
    "train_dataset = split_again['train']\n",
    "validation_dataset = split_again['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label_sexist'],\n",
      "        num_rows: 16287\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label_sexist'],\n",
      "        num_rows: 1810\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label_sexist'],\n",
      "        num_rows: 2011\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset,\n",
    "})\n",
    "print(dataset_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token read successfully: 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\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"credentials.json\"\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    token_data = json.load(json_file)\n",
    "\n",
    "token = token_data.get(\"huggingface_token\")\n",
    "if token:\n",
    "    print(f\"Token read successfully: {token}\")\n",
    "else:\n",
    "    print(\"Error: Token not found in the JSON file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/yzheng/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1325c60f617f414b895e9bfc1f463a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a06bb8a6bb4eccb42d89dedfe2b2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55afe3c3c14a4467bc48804661571e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7cefbea29043f0a284c7dda5e39e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "BadRequestError",
     "evalue": " (Request ID: Root=1-65724fa1-35953d9b70242b422fb35d36;99db7a38-7aef-4592-a1da-2699d6e577b8)\n\nBad request for commit endpoint:\nYour push was rejected because it contains binary files. Please use https://git-lfs.github.com/ to store binary files. See also: https://hf.co/docs/hub/repositories-getting-started#terminal Offending files: - data/train-00000-of-00001.parquet (ref: refs/heads/main)\nYour push was rejected because it contains binary files. Please use https://git-lfs.github.com/ to store binary files. See also: https://hf.co/docs/hub/repositories-getting-started#terminal  Offending files:   - data/train-00000-of-00001.parquet (ref: refs/heads/main)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:270\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    271\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://huggingface.co/api/datasets/yangezheng/sexism/commit/main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/yzheng/projects/chatguardians/datasets/modifyDatasets.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwsl/home/yzheng/projects/chatguardians/datasets/modifyDatasets.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwsl/home/yzheng/projects/chatguardians/datasets/modifyDatasets.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m huggingface_hub\u001b[39m.\u001b[39mlogin(token)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bwsl/home/yzheng/projects/chatguardians/datasets/modifyDatasets.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m split_dataset\u001b[39m.\u001b[39;49mpush_to_hub(\u001b[39m\"\u001b[39;49m\u001b[39myangezheng/sexism\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/datasets/dataset_dict.py:1772\u001b[0m, in \u001b[0;36mDatasetDict.push_to_hub\u001b[0;34m(self, repo_id, config_name, commit_message, private, token, revision, branch, create_pr, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   1770\u001b[0m commit_message \u001b[39m=\u001b[39m commit_message \u001b[39mif\u001b[39;00m commit_message \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUpload dataset\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1771\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(additions) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mUPLOADS_MAX_NUMBER_PER_COMMIT:\n\u001b[0;32m-> 1772\u001b[0m     api\u001b[39m.\u001b[39;49mcreate_commit(\n\u001b[1;32m   1773\u001b[0m         repo_id,\n\u001b[1;32m   1774\u001b[0m         operations\u001b[39m=\u001b[39;49madditions \u001b[39m+\u001b[39;49m deletions,\n\u001b[1;32m   1775\u001b[0m         commit_message\u001b[39m=\u001b[39;49mcommit_message,\n\u001b[1;32m   1776\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1777\u001b[0m         repo_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1778\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1779\u001b[0m         create_pr\u001b[39m=\u001b[39;49mcreate_pr,\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1783\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of files to upload is larger than \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39mUPLOADS_MAX_NUMBER_PER_COMMIT\u001b[39m}\u001b[39;00m\u001b[39m. Splitting the push into multiple commits.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1784\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/huggingface_hub/hf_api.py:1045\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_as_future(fn, \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1044\u001b[0m \u001b[39m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1045\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/huggingface_hub/hf_api.py:3237\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   3236\u001b[0m     commit_resp \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mpost(url\u001b[39m=\u001b[39mcommit_url, headers\u001b[39m=\u001b[39mheaders, data\u001b[39m=\u001b[39mdata, params\u001b[39m=\u001b[39mparams)\n\u001b[0;32m-> 3237\u001b[0m     hf_raise_for_status(commit_resp, endpoint_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcommit\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3238\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   3239\u001b[0m     e\u001b[39m.\u001b[39mappend_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:326\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n\u001b[1;32m    323\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    324\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mBad request for \u001b[39m\u001b[39m{\u001b[39;00mendpoint_name\u001b[39m}\u001b[39;00m\u001b[39m endpoint:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m endpoint_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mBad request:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    325\u001b[0m     )\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(message, response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39mraise\u001b[39;00m HfHubHTTPError(\u001b[39mstr\u001b[39m(e), response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m:  (Request ID: Root=1-65724fa1-35953d9b70242b422fb35d36;99db7a38-7aef-4592-a1da-2699d6e577b8)\n\nBad request for commit endpoint:\nYour push was rejected because it contains binary files. Please use https://git-lfs.github.com/ to store binary files. See also: https://hf.co/docs/hub/repositories-getting-started#terminal Offending files: - data/train-00000-of-00001.parquet (ref: refs/heads/main)\nYour push was rejected because it contains binary files. Please use https://git-lfs.github.com/ to store binary files. See also: https://hf.co/docs/hub/repositories-getting-started#terminal  Offending files:   - data/train-00000-of-00001.parquet (ref: refs/heads/main)"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login(token)\n",
    "\n",
    "dataset_dict.push_to_hub(\"yangezheng/sexism\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
